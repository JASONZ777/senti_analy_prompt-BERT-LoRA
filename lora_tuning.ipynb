{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JASONZ777/senti_analy_prompt-BERT/blob/main/lora_tuning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D30DQhuyan3o"
      },
      "source": [
        "# Method 2： LoRA-tuning"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "import os\n",
        "os.chdir('/content/gdrive/MyDrive/sentiment-analysis')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yyKyLmrlbFjf",
        "outputId": "a1f54287-f731-4b40-c8f2-06048cf3482b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uFLI6WMZan3q"
      },
      "outputs": [],
      "source": [
        "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class upload_dataset(Dataset):\n",
        "    def __init__(self, data_file):\n",
        "        self.data = self.load_data(data_file)\n",
        "\n",
        "    def load_data(self, data_file):\n",
        "        Data = {}\n",
        "        with open(data_file, 'rt', encoding='utf-8') as f:\n",
        "            for idx, line in enumerate(f):\n",
        "                items = line.strip().split('\\t')\n",
        "                assert len(items) == 2\n",
        "                Data[idx] = {\n",
        "                    'comment': items[0],\n",
        "                    'label': int(items[1]),\n",
        "                }\n",
        "        return Data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P1dwcXe8an3s",
        "outputId": "4d70431d-231b-4086-a5d8-734b31346a0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train set size: 9600\n",
            "valid set size: 1200\n",
            "test set size: 1200\n",
            "{'comment': '选择珠江花园的原因就是方便，有电动扶梯直接到达海边，周围餐馆、食廊、商场、超市、摊位一应俱全。酒店装修一般，但还算整洁。 泳池在大堂的屋顶，因此很小，不过女儿倒是喜欢。 包的早餐是西式的，还算丰富。 服务吗，一般', 'label': 1}\n"
          ]
        }
      ],
      "source": [
        "# testing\n",
        "train_data = upload_dataset('chnsenticorp/train/part.0')\n",
        "valid_data = upload_dataset('chnsenticorp/dev/part.0')\n",
        "test_data = upload_dataset('chnsenticorp/test/part.0')\n",
        "print(f'train set size: {len(train_data)}')\n",
        "print(f'valid set size: {len(valid_data)}')\n",
        "print(f'test set size: {len(test_data)}')\n",
        "print(next(iter(train_data)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uck7rWEnan3s"
      },
      "outputs": [],
      "source": [
        "def map_label(tokenizer):\n",
        "    return {\n",
        "        '1': {'token': '好', 'id': tokenizer.convert_tokens_to_ids(\"好\")},\n",
        "        '0': {'token': '差', 'id': tokenizer.convert_tokens_to_ids(\"差\")}\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JLbP_Qz0an3s",
        "outputId": "e92ea9d7-1654-4d73-a6a0-319086c88a80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import AutoTokenizer\n",
        "checkpoint = 'bert-base-chinese'\n",
        "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "# find the id of the label defined in tokenizer\n",
        "label = map_label(tokenizer)\n",
        "pos_id, neg_id = label['1']['id'], label['0']['id']\n",
        "# Dataloader, in NLP we usually use collate_fn to do the padding to make sure samples have the same sequence length\n",
        "def collate(batch_samples): # operate on each batch\n",
        "    batch_sentence = []\n",
        "    batch_label = []\n",
        "    batch_mask_id = []\n",
        "    max_length = 0\n",
        "    for sample in batch_samples:\n",
        "        batch_sentence.append(sample['comment'])\n",
        "        encoding = tokenizer(sample['comment'], truncation=True)\n",
        "        max_length = max(max_length, len(encoding.tokens())) # dynamic padding to the longest\n",
        "        batch_label.append(sample['label'])\n",
        "    batch_inputs = tokenizer(batch_sentence, max_length=max_length, padding=True, truncation=True, return_tensors=\"pt\")\n",
        "    label_id = [neg_id, pos_id]\n",
        "    return {\n",
        "        'batch_inputs':batch_inputs,\n",
        "        'label_id': label_id,\n",
        "        'labels': batch_label\n",
        "\n",
        "    }\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FErEZU9Han3t",
        "outputId": "49136e18-b54b-49b9-d74d-6893ab779a95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.10/dist-packages (0.42.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from bitsandbytes) (1.11.4)\n",
            "Requirement already satisfied: numpy<1.28.0,>=1.21.6 in /usr/local/lib/python3.10/dist-packages (from scipy->bitsandbytes) (1.25.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install -q peft\n",
        "!pip install -q evaluate\n",
        "!pip install bitsandbytes\n",
        "!pip install -q accelerate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qVhXB61aan3t",
        "outputId": "e7e23794-4339-4f08-a18e-61beeb789847"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "from peft import LoraConfig, TaskType, get_peft_model\n",
        "from transformers import BertForSequenceClassification, BitsAndBytesConfig\n",
        "lora_config = LoraConfig(\n",
        "    task_type=TaskType.SEQ_CLS, r=1, lora_alpha=1, lora_dropout=0.1\n",
        ")\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "\n",
        "base_model = BertForSequenceClassification.from_pretrained(\n",
        "    checkpoint,\n",
        "    quantization_config=bnb_config,\n",
        "    use_cache=False,\n",
        ")\n",
        "\n",
        "model = get_peft_model(base_model, lora_config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nAe9fFzban3t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "daf9b7dd-1bb0-43e3-f132-e66177fe79ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainable params: 38402 || all params: 59545348 || trainable%: 0.06449202379336166\n"
          ]
        }
      ],
      "source": [
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )\n",
        "\n",
        "print_trainable_parameters(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5e9lOQxEan3t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe58225f-64a8-499c-8470-bd550456ef37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import AdamW\n",
        "from tqdm import tqdm\n",
        "# Hyper-parameters\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# put data to GPU\n",
        "def to_device(batch_data):\n",
        "    new_batch_data = {}\n",
        "    for k, v in batch_data.items():\n",
        "        if k == 'batch_inputs':\n",
        "            new_batch_data[k] = {\n",
        "                k_: v_.to(device) for k_, v_ in v.items()\n",
        "            }\n",
        "        elif k == 'label_id':\n",
        "            new_batch_data[k] = v\n",
        "        else:\n",
        "            new_batch_data[k] = torch.tensor(v).to(device)\n",
        "    return new_batch_data\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=4e-5)\n",
        "loss_fun = torch.nn.CrossEntropyLoss()\n",
        "num_epoch = 10\n",
        "batch_size = 16\n",
        "\n",
        "# encapsulate into the dataloader as input\n",
        "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=collate)\n",
        "valid_loader = DataLoader(valid_data, batch_size=batch_size, shuffle=True, collate_fn=collate)\n",
        "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True, collate_fn=collate)\n",
        "\n",
        "# logs\n",
        "loss_hists = {'train':[],'val': []}\n",
        "acc_hists = {'train':[],'val': []}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QQmUG6BJan3u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fdd7ad3-203e-40c7-b9e5-e68308849a71"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10:  17%|█▋        | 103/600 [01:17<05:05,  1.63it/s]"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "for epoch in range(num_epoch):\n",
        "    train_loss = 0\n",
        "    i = 1\n",
        "    correct_num = 0\n",
        "    num_batch = 0\n",
        "    for batch_data in tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{num_epoch}\"):\n",
        "      if num_batch < 600:\n",
        "        model.train() # training mode\n",
        "        batch_data = to_device(batch_data)\n",
        "        output = model(**batch_data['batch_inputs'])\n",
        "        output = output.logits\n",
        "        loss = loss_fun(output, batch_data['labels'])\n",
        "        loss.backward()\n",
        "        train_loss+=loss.item()\n",
        "\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()# set gradient to 0 when batch is updated\n",
        "\n",
        "\n",
        "        # if i % 10 == 0: # record every 10 training batches\n",
        "        #     output = output.argmax(dim=1)\n",
        "        #     acc = (output == batch_data['labels']).sum().item()/ len(batch_data['labels'])\n",
        "        #     print(f'Training stage:Batch {i}, Loss: {loss.item()}, Accuracy: {acc}')\n",
        "\n",
        "        # if i % 30 == 0: # validate every 30 batches\n",
        "        #     model.eval()\n",
        "        #     batch_val = next(iter(valid_loader))\n",
        "        #     with torch.no_grad():\n",
        "        #         batch_val = to_device(batch_val)\n",
        "        #         output = model(**batch_val)\n",
        "        #     val_loss = loss_fun(output, torch.tensor(batch_val['labels']))\n",
        "        #     output = output.argmax(dim=1)\n",
        "        #     acc = (output == torch.tensor(batch_val['labels'])).sum().item()/ len(batch_val['labels'])\n",
        "        #     print(f'Validation stage:Batch {i}, Loss: {loss.item()}, Accuracy: {acc}')\n",
        "\n",
        "        # i+=1\n",
        "        num_batch+=1\n",
        "        output = output.argmax(dim=1)\n",
        "        correct_num += (output == batch_data['labels']).sum().item()\n",
        "      else:\n",
        "        break\n",
        "\n",
        "    acc = correct_num/(num_batch*batch_size)\n",
        "    average_loss = train_loss/num_batch\n",
        "    loss_hists['train'].append(average_loss)\n",
        "    acc_hists['train'].append(acc)\n",
        "    print(f\"Epoch {epoch + 1}/{num_epoch}, Training Loss: {average_loss:.4f}, Training accuracy: {acc:.4f}\")\n",
        "\n",
        "    # validation\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        val_loss = 0\n",
        "        correct_num = 0\n",
        "        for i, batch_data in enumerate(valid_loader):\n",
        "            batch_data = to_device(batch_data)\n",
        "            output = model(**batch_data['batch_inputs'])\n",
        "            output = output.logits\n",
        "            loss = loss_fun(output, batch_data['labels'])\n",
        "            output = output.argmax(dim=1)\n",
        "\n",
        "            correct_num += (output == batch_data['labels']).sum().item()\n",
        "            val_loss+=loss\n",
        "\n",
        "        acc = correct_num/ (len(valid_loader)*batch_size)\n",
        "        average_val_loss = val_loss/len(valid_loader)\n",
        "\n",
        "        loss_hists['val'].append(average_val_loss)\n",
        "        acc_hists['val'].append(acc)\n",
        "        print(f'Epoch {epoch + 1}/{num_epoch}, Validation loss: {average_val_loss:.4f}, Validation accuracy: {acc:.4f}')\n",
        "\n",
        "\n",
        "# draw the loss figures\n",
        "fig, ax = plt.subplots(1,2)\n",
        "ax[0].plot(torch.tensor(loss_hists['train']).cpu().detach().numpy()) # can only matplotlib to plot numpy on CPU\n",
        "ax[0].plot(torch.tensor(loss_hists['val']).cpu().detach().numpy())\n",
        "ax[0].set_title('Loss-Epoch')\n",
        "ax[0].set_xlabel('Epoch')\n",
        "ax[0].set_ylabel('Loss')\n",
        "ax[0].legend(['Training loss', 'Validation loss'])\n",
        "\n",
        "ax[1].plot(torch.tensor(acc_hists['train']).cpu().detach().numpy())\n",
        "ax[1].plot(torch.tensor(acc_hists['val']).cpu().detach().numpy())\n",
        "ax[1].set_title('Acc-Epoch')\n",
        "ax[1].set_xlabel('Epoch')\n",
        "ax[1].set_ylabel('Accuracy')\n",
        "ax[1].legend(['Training acc', 'Validation acc'])\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}